{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"11o0AjRqEP1mAk-OIbIU6n74czFa0ld1P","timestamp":1683298538152}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"2A5zM9AfWdqx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DRg6bjBQwpcj"},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","source":["# Load the dataset\n","dataset_path = \"/content/drive/MyDrive/Research/NLP/Project/Identifying_offensive_text_with_Bengla_language_from_social_media.xlsx\"\n","df = pd.read_excel(dataset_path)\n","\n","length_yes = len(df[df.threat_label=='yes'])\n","length_no =len(df[df.threat_label=='no'])\n","print(length_yes, length_no)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PM23sf-7UMK1","executionInfo":{"status":"ok","timestamp":1682835723006,"user_tz":-360,"elapsed":13,"user":{"displayName":"Md. Mahadi Hasan Sany","userId":"17757769576657607417"}},"outputId":"163080d2-21c1-40cb-d9e1-8a9b1bdc7cdf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["947 3976\n"]}]},{"cell_type":"code","source":["# Get the maximum length of text in the 'text_column' column\n","max_length = df['comment'].str.len().max()\n","\n","print('Maximum text length:', max_length)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VVEtWORDaIfL","executionInfo":{"status":"ok","timestamp":1682835723006,"user_tz":-360,"elapsed":10,"user":{"displayName":"Md. Mahadi Hasan Sany","userId":"17757769576657607417"}},"outputId":"7fb2363e-509d-412a-8b0b-387bb3995611"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum text length: 1296\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import train_test_split\n","import random\n","import time\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', truncation=True)\n","\n","class BanglaNewsDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_length):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.text = dataframe.comment\n","        self.targets = self.data.threat_label\n","        self.max_length = max_length\n","        \n","    def __len__(self):\n","        return len(self.text)\n","    \n","    def __getitem__(self, index):\n","        text = str(self.text[index])\n","        text = \" \".join(text.split())\n","        labels = self.targets[index]\n","        \n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_token_type_ids=True,\n","            return_tensors='pt'\n","            \n","        )\n","        \n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs['token_type_ids']\n","        \n","        return ids.squeeze(), mask.squeeze(), token_type_ids.squeeze(), labels\n","\n","max_length = 256\n","batch_size = 16"],"metadata":{"id":"De7_D6BwLxps"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data, test_data = train_test_split(df, test_size=0.10, random_state=42)\n","train_data, val_data = train_test_split(train_data, test_size=0.20, random_state=42)\n","\n","train_data = train_data.reset_index(drop=True)\n","val_data = val_data.reset_index(drop=True)\n","test_data = test_data.reset_index(drop=True)\n","\n","train_dataset = BanglaNewsDataset(train_data, tokenizer, max_length)\n","val_dataset = BanglaNewsDataset(val_data, tokenizer, max_length)\n","test_dataset = BanglaNewsDataset(test_data, tokenizer, max_length)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"],"metadata":{"id":"V2-Wm2fHs9wg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BanglaNewsClassifier(torch.nn.Module):\n","    def __init__(self, num_classes):\n","        super(BanglaNewsClassifier, self).__init__()\n","        self.bert = BertForSequenceClassification.from_pretrained('bert-base-multilingual-uncased', num_labels=num_classes)\n","        self.dropout = torch.nn.Dropout(0.1)\n","        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, num_classes)\n","        # print(\"\\n\\n\\n\\n\", self.bert.config.hidden_size, \"\\n\\n\\n\\n\\n\")\n","        \n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        \n","        return outputs.logits"],"metadata":{"id":"qeGpAKaGMZTF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_classes = len(df.threat_label.unique())"],"metadata":{"id":"eqIfpXFOMgyD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","save_model = BanglaNewsClassifier(num_classes).to(device)\n","# Load the saved model weights\n","save_model.load_state_dict(torch.load('/content/drive/MyDrive/model_weights.pth'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q7Q418CByqZe","executionInfo":{"status":"ok","timestamp":1682835726403,"user_tz":-360,"elapsed":2478,"user":{"displayName":"Md. Mahadi Hasan Sany","userId":"17757769576657607417"}},"outputId":"b3702602-b974-4e93-b2f5-de639a9b54e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["def predict_class(save_model, tokenizer, device, sentence):\n","    # Tokenize the input sentence\n","    encoded_sent = tokenizer.encode_plus(\n","        sentence,\n","        add_special_tokens=True,\n","        max_length=128,\n","        padding='max_length',\n","        return_attention_mask=True,\n","        return_token_type_ids=True,\n","        return_tensors='pt'\n","    )\n","\n","    # Move the input to the correct device\n","    input_ids = encoded_sent['input_ids'].to(device)\n","    attention_mask = encoded_sent['attention_mask'].to(device)\n","    token_type_ids = encoded_sent['token_type_ids'].to(device)\n","\n","    # Make the prediction\n","    with torch.no_grad():\n","        outputs = save_model(input_ids, attention_mask, token_type_ids)\n","        predictions = torch.argmax(outputs, dim=1)\n","    \n","    return predictions.item()"],"metadata":{"id":"vhFbzgAIyrqc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence = str(input())\n","predicted_class = predict_class(save_model, tokenizer, device, sentence)\n","# print(type(predicted_class))\n","if predicted_class == 1:\n","  print(\"This sentence is threatful!!!\")\n","else:\n","  print(\"This sentence is not threatful...!\")"],"metadata":{"id":"42vtfu-HKeeL","executionInfo":{"status":"ok","timestamp":1682835730777,"user_tz":-360,"elapsed":4388,"user":{"displayName":"Md. Mahadi Hasan Sany","userId":"17757769576657607417"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"987fd37e-003b-4567-eefe-8629fcb00af1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["আজকের খবর: বৃষ্টির কারণে ঢাকায় জলবায়ু উন্নয়ন সফর থাম\n","This sentence is not threatful...!\n"]}]}]}